{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7BS4kNpJdrbKvisFwr6Uv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpv84ete5oDK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import wasserstein_distance, entropy\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#there's an issue with the size of the logits space, different for opt-125m and gpt2. 50272 is facebook/opt125m. 50257 for gpt2-xl\n",
        "\n",
        "def get_aligned_logits(text, model_name='facebook/opt-125m'):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    logits_t0_to_k_minus_2 = logits[:, :-1, :]\n",
        "    tokens_t1_to_k_minus_1 = input_ids[:, 1:]\n",
        "\n",
        "    return logits_t0_to_k_minus_2, tokens_t1_to_k_minus_1\n",
        "\n",
        "def logits_to_probabilities(logits,temp):\n",
        "    probabilities = F.softmax(logits*(1/temp), dim=-1)\n",
        "    return probabilities\n",
        "\n",
        "def get_rank(probs, token_index):\n",
        "    rank=1\n",
        "    for i in range(len(probs)):\n",
        "        if probs[i] > probs[token_index]:\n",
        "            rank+=1\n",
        "    return rank\n",
        "\n",
        "def process_text(text, model_name='facebook/opt-125m',temp=1, initial_cutoff=0, final_cutoff=500):\n",
        "\n",
        "    logits, input_ids = get_aligned_logits(text, model_name)\n",
        "    probabilities = logits_to_probabilities(logits,temp)\n",
        "\n",
        "    if model_name=='gpt2-xl':\n",
        "        vocab_size=50257\n",
        "    if model_name=='facebook/opt-125m':\n",
        "        vocab_size=50272\n",
        "\n",
        "    chosen_token_rank_list = []\n",
        "    chosen_token_prob_list = []\n",
        "    node_entropy_list=[]\n",
        "    node_variance_list=[]\n",
        "    node_tempnorm_list=[]\n",
        "\n",
        "    for i in range(initial_cutoff,min(probabilities.size(1),final_cutoff)):\n",
        "        all_token_probs = probabilities[0, i, :]\n",
        "        token_id = input_ids[0, i]\n",
        "        all_token_probs_list = all_token_probs.tolist()\n",
        "\n",
        "        chosen_token_prob = all_token_probs[token_id]\n",
        "        chosen_token_rank= get_rank(all_token_probs_list, token_id)\n",
        "        node_entropy = stats.entropy(all_token_probs_list)\n",
        "        node_variance=np.sum(all_token_probs_list*((np.log(all_token_probs_list) + node_entropy)**2))\n",
        "\n",
        "\n",
        "        tempnorm=0\n",
        "        for j in range(len(all_token_probs_list)):\n",
        "            tempnorm+=all_token_probs_list[j]**(1/temp)\n",
        "\n",
        "        chosen_token_rank_list.append(chosen_token_rank)\n",
        "        chosen_token_prob_list.append(chosen_token_prob)\n",
        "        node_entropy_list.append(node_entropy)\n",
        "        node_variance_list.append(node_variance)\n",
        "        node_tempnorm_list.append(tempnorm)\n",
        "    return chosen_token_rank_list, chosen_token_prob_list, node_entropy_list, node_variance_list, node_tempnorm_list"
      ],
      "metadata": {
        "id": "5bNguQRB5uQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_batch(texts, model_name='facebook/opt-125m',temp=1, initial_cutoff=0, final_cutoff=500):\n",
        "  text_batch_data={}\n",
        "  for i in range(len(texts)):\n",
        "    print(i)\n",
        "    chosen_token_rank_list, chosen_token_prob_list, node_entropy_list, node_variance_list, node_tempnorm_list = process_text(texts[i], model_name, temp, initial_cutoff, final_cutoff)\n",
        "    text_batch_data[i]={}\n",
        "    text_batch_data[i]['chosen_token_rank_list']=chosen_token_rank_list\n",
        "    text_batch_data[i]['chosen_token_prob_list']=chosen_token_prob_list\n",
        "    text_batch_data[i]['node_entropy_list']=node_entropy_list\n",
        "    text_batch_data[i]['node_variance_list']=node_variance_list\n",
        "    text_batch_data[i]['node_tempnorm_list']=node_tempnorm_list\n",
        "  return text_batch_data"
      ],
      "metadata": {
        "id": "UKkdVAww5xMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_detect_score(chosen_token_prob_list, node_entropy_list, node_variance_list, initial_cutoff=30, final_cutoff=200):\n",
        "  final_variance=0\n",
        "  final_log_prob=0\n",
        "  final_entropy=0\n",
        "  for i in range(initial_cutoff, min(len(chosen_token_prob_list), final_cutoff)):\n",
        "    final_variance+=node_variance_list[i]\n",
        "    final_log_prob+=np.log(chosen_token_prob_list[i])\n",
        "    final_entropy+=node_entropy_list[i]\n",
        "\n",
        "  fd_score=(final_log_prob+final_entropy)/((final_variance)**0.5)\n",
        "  return fd_score, final_variance, final_log_prob, final_entropy"
      ],
      "metadata": {
        "id": "7b7iRNit50Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temptest_score(chosen_token_log_prob_list, node_tempnorm_list, temp, initial_cutoff=30, final_cutoff=200):\n",
        "  temptest_scores=[]\n",
        "  for i in range(initial_cutoff, min(len(chosen_token_log_prob_list), final_cutoff)):\n",
        "    temptest_scores.append((-((1/temp)-1)*chosen_token_log_prob_list[i]+np.log(node_tempnorm_list[i])))\n",
        "  final_temptest_score=np.mean(temptest_scores)\n",
        "  return temptest_scores, final_temptest_score"
      ],
      "metadata": {
        "id": "LSkyQh2c53aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_text_batch(text_batch_data ,temp=1, initial_cutoff=30, final_cutoff=200):\n",
        "  scoring_data={}\n",
        "  for i in range(len(text_batch_data)):\n",
        "    chosen_token_rank_list=text_batch_data[i]['chosen_token_rank_list']\n",
        "    chosen_token_prob_list=text_batch_data[i]['chosen_token_prob_list']\n",
        "    node_entropy_list=text_batch_data[i]['node_entropy_list']\n",
        "    node_variance_list=text_batch_data[i]['node_variance_list']\n",
        "    node_tempnorm_list=text_batch_data[i]['node_tempnorm_list']\n",
        "    fd_score, final_variance, final_log_prob, final_entropy=fast_detect_score(chosen_token_prob_list, node_entropy_list, node_variance_list, initial_cutoff, final_cutoff)\n",
        "    temptest_scores, final_temptest_score=temptest_score(chosen_token_prob_list, node_tempnorm_list, temp, initial_cutoff, final_cutoff)\n",
        "    scoring_data[i]={}\n",
        "    scoring_data[i]['fd_score']=fd_score\n",
        "    scoring_data[i]['final_temptest_score']=final_temptest_score\n",
        "  return scoring_data"
      ],
      "metadata": {
        "id": "kCwDZ7Md56d3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}